{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16 is a convolutional neural network (CNN) architecture\n",
    "#Visual Geometry Group (VGG)\n",
    "#It is a deep network with 16 layers (13 convolutional layers and 3 fully connected layers) and is known for its simplicity and effectiveness in image classification tasks.\n",
    "\n",
    "#The VGG16 model was trained on the ImageNet dataset, which contains over 14 million images and 1,000 classes.\n",
    "\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138357544 (527.79 MB)\n",
      "Trainable params: 138357544 (527.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image from file\n",
    "#The target_size parameter resizes the image to 224x224 pixels, which is the input size required by the VGG16 model.\n",
    "image = load_img('car.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "#This transformation is necessary because deep learning models in Keras expect input data in the form of NumPy arrays.\n",
    "image = img_to_array(image)\n",
    "# reshape data for the model\n",
    "#reshapes the image array to add an extra dimension at the beginning, which represents the batch size.\n",
    "#The VGG16 model expects input of shape (batch_size, height, width, channels)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 7.6060997e+01,  6.2221001e+01,  5.1320000e+01],\n",
       "         [ 7.8060997e+01,  6.4221001e+01,  5.3320000e+01],\n",
       "         [ 7.8060997e+01,  6.4221001e+01,  5.3320000e+01],\n",
       "         ...,\n",
       "         [-3.4939003e+01, -5.4778999e+01, -4.6680000e+01],\n",
       "         [-6.8939003e+01, -9.4778999e+01, -7.5680000e+01],\n",
       "         [-5.0939003e+01, -6.0778999e+01, -6.2680000e+01]],\n",
       "\n",
       "        [[ 7.6060997e+01,  6.2221001e+01,  5.1320000e+01],\n",
       "         [ 7.8060997e+01,  6.4221001e+01,  5.3320000e+01],\n",
       "         [ 7.8060997e+01,  6.4221001e+01,  5.3320000e+01],\n",
       "         ...,\n",
       "         [-3.4939003e+01, -5.1778999e+01, -3.9680000e+01],\n",
       "         [-6.8939003e+01, -9.1778999e+01, -6.8680000e+01],\n",
       "         [-3.9939003e+01, -5.2778999e+01, -4.5680000e+01]],\n",
       "\n",
       "        [[ 7.6060997e+01,  6.2221001e+01,  5.1320000e+01],\n",
       "         [ 7.8060997e+01,  6.4221001e+01,  5.3320000e+01],\n",
       "         [ 7.8060997e+01,  6.4221001e+01,  5.3320000e+01],\n",
       "         ...,\n",
       "         [-5.0939003e+01, -6.5778999e+01, -4.3680000e+01],\n",
       "         [-7.4939003e+01, -9.1778999e+01, -6.3680000e+01],\n",
       "         [-3.3939003e+01, -4.6778999e+01, -3.5680000e+01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 9.0609970e+00, -3.7789993e+00, -1.0680000e+01],\n",
       "         [ 9.0609970e+00, -3.7789993e+00, -1.0680000e+01],\n",
       "         [ 9.0609970e+00, -3.7789993e+00, -1.0680000e+01],\n",
       "         ...,\n",
       "         [ 2.0609970e+00, -6.7789993e+00, -1.2680000e+01],\n",
       "         [ 2.0609970e+00, -6.7789993e+00, -1.2680000e+01],\n",
       "         [ 2.0609970e+00, -6.7789993e+00, -1.2680000e+01]],\n",
       "\n",
       "        [[ 8.0609970e+00, -4.7789993e+00, -1.1680000e+01],\n",
       "         [ 8.0609970e+00, -4.7789993e+00, -1.1680000e+01],\n",
       "         [ 8.0609970e+00, -4.7789993e+00, -1.1680000e+01],\n",
       "         ...,\n",
       "         [ 6.0997009e-02, -8.7789993e+00, -1.4680000e+01],\n",
       "         [ 6.0997009e-02, -8.7789993e+00, -1.4680000e+01],\n",
       "         [-9.3900299e-01, -9.7789993e+00, -1.5680000e+01]],\n",
       "\n",
       "        [[ 8.0609970e+00, -4.7789993e+00, -1.1680000e+01],\n",
       "         [ 8.0609970e+00, -4.7789993e+00, -1.1680000e+01],\n",
       "         [ 8.0609970e+00, -4.7789993e+00, -1.1680000e+01],\n",
       "         ...,\n",
       "         [-9.3900299e-01, -9.7789993e+00, -1.5680000e+01],\n",
       "         [-9.3900299e-01, -9.7789993e+00, -1.5680000e+01],\n",
       "         [-3.9390030e+00, -1.2778999e+01, -1.8680000e+01]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "#yhat will be a NumPy array containing the predicted probabilities for each of the 1000 classes in the ImageNet dataset.\n",
    "\n",
    "yhat = model.predict(image)\n",
    "# convert the probabilities to class labels\n",
    "#decode_predictions(yhat) converts the predicted probabilities into human-readable class labels.\n",
    "label = decode_predictions(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('n04285008', 'sports_car', 0.9651953),\n",
       "  ('n03100240', 'convertible', 0.021941917),\n",
       "  ('n03459775', 'grille', 0.006794773),\n",
       "  ('n02814533', 'beach_wagon', 0.002312458),\n",
       "  ('n04037443', 'racer', 0.0020715655)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sports_car (96.52%)\n"
     ]
    }
   ],
   "source": [
    "label = label[0][0]\n",
    "# print the classification\n",
    "print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
